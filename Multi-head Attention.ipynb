{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073cf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc17552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    " \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    " \n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    " \n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c304680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    " \n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    " \n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    " \n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441f115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.63356173 -0.14803727 -0.19121572 ... -0.15757044 -0.12101033\n",
      "   -0.05706044]\n",
      "  [ 0.63226557 -0.14610596 -0.19096828 ... -0.1583976  -0.12066325\n",
      "   -0.05740909]\n",
      "  [ 0.63303465 -0.14726456 -0.1915228  ... -0.15749091 -0.12169603\n",
      "   -0.0546914 ]\n",
      "  [ 0.6388082  -0.14934358 -0.19321346 ... -0.15629281 -0.12314503\n",
      "   -0.05891854]\n",
      "  [ 0.63546985 -0.14869155 -0.19255193 ... -0.15596199 -0.119051\n",
      "   -0.05900563]]\n",
      "\n",
      " [[ 0.57743484 -0.27558616 -0.02263549 ... -0.20780912 -0.08700554\n",
      "   -0.09953242]\n",
      "  [ 0.5781104  -0.27140155 -0.02565989 ... -0.20409004 -0.08703464\n",
      "   -0.09857348]\n",
      "  [ 0.57622045 -0.2733061  -0.02417435 ... -0.20444727 -0.0873732\n",
      "   -0.10202871]\n",
      "  [ 0.57951736 -0.2743711  -0.02290831 ... -0.20593756 -0.0880878\n",
      "   -0.09948993]\n",
      "  [ 0.57856786 -0.2699763  -0.02674772 ... -0.20430014 -0.08570696\n",
      "   -0.09831969]]\n",
      "\n",
      " [[ 0.6253058  -0.18317097 -0.15806416 ... -0.06697564 -0.09985926\n",
      "   -0.08107325]\n",
      "  [ 0.6248526  -0.18278638 -0.15840401 ... -0.0647218  -0.0978322\n",
      "   -0.07958306]\n",
      "  [ 0.6224912  -0.1821704  -0.16119328 ... -0.06330136 -0.0956347\n",
      "   -0.08294212]\n",
      "  [ 0.6269246  -0.18015051 -0.16149344 ... -0.064752   -0.09907264\n",
      "   -0.07935897]\n",
      "  [ 0.62445194 -0.1825857  -0.16025148 ... -0.06574487 -0.09699163\n",
      "   -0.07979912]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.56057996 -0.3257072  -0.13117728 ... -0.27718696  0.04972923\n",
      "   -0.15726489]\n",
      "  [ 0.5612846  -0.3206704  -0.13339628 ... -0.27787948  0.05018874\n",
      "   -0.15604621]\n",
      "  [ 0.5642966  -0.3261331  -0.1311053  ... -0.279429    0.05082742\n",
      "   -0.15776902]\n",
      "  [ 0.56135535 -0.32381016 -0.1297889  ... -0.27621216  0.05183278\n",
      "   -0.15709624]\n",
      "  [ 0.5608222  -0.32156476 -0.12882136 ... -0.2756667   0.05385483\n",
      "   -0.16030942]]\n",
      "\n",
      " [[ 0.55883694 -0.16121268 -0.01864303 ... -0.17558293 -0.05085403\n",
      "    0.04057535]\n",
      "  [ 0.55702066 -0.1603917  -0.01648463 ... -0.1754451  -0.04759419\n",
      "    0.03954789]\n",
      "  [ 0.55746704 -0.16060576 -0.01847901 ... -0.17494915 -0.05078704\n",
      "    0.04142905]\n",
      "  [ 0.55371064 -0.16035047 -0.01568282 ... -0.17563954 -0.04643331\n",
      "    0.04141198]\n",
      "  [ 0.5560497  -0.1614888  -0.0172433  ... -0.17523962 -0.05093234\n",
      "    0.04200738]]\n",
      "\n",
      " [[ 0.6313477  -0.13632198 -0.08787241 ... -0.18267733 -0.05470278\n",
      "   -0.12470927]\n",
      "  [ 0.630627   -0.13690187 -0.08616185 ... -0.1813573  -0.05235175\n",
      "   -0.12252299]\n",
      "  [ 0.62983215 -0.1368372  -0.08826707 ... -0.18175377 -0.05459122\n",
      "   -0.12443288]\n",
      "  [ 0.6288435  -0.13641402 -0.08425906 ... -0.18121554 -0.05283954\n",
      "   -0.12382121]\n",
      "  [ 0.6303047  -0.13538605 -0.08717207 ... -0.18117163 -0.05195646\n",
      "   -0.12157471]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    " \n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process\n",
    " \n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    " \n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "print(multihead_attention(queries, keys, values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
